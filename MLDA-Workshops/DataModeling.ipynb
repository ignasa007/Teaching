{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# For dataframe manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# To make plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To set the theme of the plot\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# To perform PCA, and sparce coding\n",
    "from sklearn.decomposition import PCA, SparseCoder, DictionaryLearning\n",
    "\n",
    "# People faces dataset\n",
    "from sklearn.datasets import fetch_lfw_people, make_sparse_coded_signal, load_breast_cancer\n",
    "\n",
    "# To split the data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For standardizing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# To perofrm KNN clustering\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# To perform classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# To perform SVD\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# For making the sparse matrix\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "# For comparing training times\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors Classification\n",
    "KNN is a non-parametric, lazy learning algorithm. When we say a technique is non-parametric, it means that it does not make any assumptions about the underlying data. In other words, it makes its selection based off of the proximity to other data points regardless of what feature the numerical values represent.\n",
    "\n",
    "Let’s take a look at how we could go about classifying data using the K-Nearest Neighbors algorithm in Python. For this tutorial, we’ll be using the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Making a data frame of the input and selecting two columns for this example\n",
    "X = pd.DataFrame(breast_cancer.data, columns = breast_cancer.feature_names)\n",
    "X = X[['mean area', 'mean compactness']]\n",
    "\n",
    "# Making a one-hot encoded data frame for the outputs\n",
    "y = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)\n",
    "y = pd.get_dummies(y, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the KNeighborsClassifier looks for the 5 nearest neighbors. We must explicitly tell the classifier to use Euclidean distance for determining the proximity between neighboring points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our newly trained model, we predict whether a tumor is benign or not given its mean compactness and area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visually compare the predictions made by our model with the samples inside the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='mean area',\n",
    "    y='mean compactness',\n",
    "    hue='benign',\n",
    "    data=X_test.join(y_test, how='outer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    X_test['mean area'],\n",
    "    X_test['mean compactness'],\n",
    "    c=y_pred,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data. \n",
    "\n",
    "Consider the following 200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a random number generator (rng)\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Genrating the data using rng - 200 points with x and y dimension\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "\n",
    "# Plotting the points\n",
    "plt.scatter(X[:, 0], X[:, 1], c = 'mediumblue', alpha = 1)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, it is clear that there is a nearly linear relationship between the x and y variables. Rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values.\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. We can compute thsi using Scikit-Learn's PCA estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making an object to perform PCA - n_components is the number of components to keep\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit the principal component analyzer to the training data\n",
    "pca.fit(X)\n",
    "\n",
    "# The analyzer now has values for the principal components and the fraction of variance explained by them\n",
    "for i, (vector, variance) in enumerate(zip(pca.components_, pca.explained_variance_), 1):\n",
    "    print(f'Principal Component {i} - {np.around(vector, 5)}, Explained Variance - {round(100 * variance, 5)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for drawing the principal components\n",
    "def draw_vector(v0, v1, ax = None):\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    arrow_props = {'arrowstyle': '->', 'linewidth': 2, 'shrinkA': 0, 'shrinkB': 0, 'color': 'black'}\n",
    "    ax.annotate('', v1, v0, arrowprops = arrow_props)\n",
    "\n",
    "# Making the plot\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c = 'mediumblue', alpha = 0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot these principal components beside the original data, we see the plots shown here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Making a dictionary for font properties for axes labels and title\n",
    "fontdict = {'family':'serif', 'color':'black', 'weight':'normal', 'size': 14}\n",
    "\n",
    "# Initializing the two subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14, 5))\n",
    "fig.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "# Making the plot with X and Y axis\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c = 'mediumblue', alpha = 0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax = ax[0])\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_xlabel('X', fontdict = fontdict, labelpad = 10)\n",
    "ax[0].set_ylabel('Y', fontdict = fontdict, labelpad = 10)\n",
    "ax[0].set_title('Input', fontdict = fontdict, pad = 10)\n",
    "\n",
    "# Making the plot taking the prinicpal components as the axes\n",
    "X_pca = pca.transform(X)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c = 'mediumblue', alpha = 0.2)\n",
    "draw_vector([0, 0], [0, 3], ax = ax[1])\n",
    "draw_vector([0, 0], [3, 0], ax = ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlim = (-5, 5), ylim = (-3, 3.1))\n",
    "ax[1].set_xlabel('Component 1', fontdict = fontdict, labelpad = 10)\n",
    "ax[1].set_ylabel('Component 2', fontdict = fontdict, labelpad = 10)\n",
    "ax[1].set_title('Principal Components', fontdict = fontdict, pad = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for dimensionality reduction\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of components to keep to 1\n",
    "pca = PCA(n_components = 1)\n",
    "\n",
    "# Fitting the analyzer to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transforming the data to get a reduced dimension\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the inverse transform of the one dimesnional data\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "# Plotting the original data\n",
    "plt.scatter(X[:, 0], X[:, 1], c = 'mediumblue', alpha = 0.2)\n",
    "# Plotting the projected data\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], c = 'mediumblue', alpha = 0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the data for faces\n",
    "faces = fetch_lfw_people(min_faces_per_person = 60)\n",
    "\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s visualize some images from the dataset. You can see that each image has a complete face, and the facial features like eyes, nose, and lips are clearly visible in each image. Now that we have our dataset ready, let’s compress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize = (10, 3),\n",
    "                       subplot_kw = {'xticks': [], 'yticks': []},\n",
    "                       gridspec_kw = {'wspace': 0.1})\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].imshow(faces.data[i].reshape(62, 47), cmap = 'binary_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the principal axes that span this dataset. Because this is a large dataset, we will use PCA with a randomized algorithm — it contains a randomized method to approximate the first N principal components much more quickly than the standard PCA estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000). \n",
    "\n",
    "We will keep the top $k$ templates (principal components) and drop the remaining. But, how many templates shall we keep? If we keep more templates, our reconstructed images will closely resemble the original images but we will need more storage to store the compressed data. If we keep too few templates, our reconstructed images will look very different from the original images.\n",
    "\n",
    "For this notebook, we will keep $k=150$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(150, svd_solver = 'randomized', random_state = 42)\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as \"eigenvectors,\" so these types of images are often called \"eigenfaces\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize = (10, 5), \n",
    "                         subplot_kw = {'xticks': [], 'yticks': []}, \n",
    "                         gridspec_kw = {'hspace': 0.1, 'wspace': 0.06})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap = 'bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let's take a look at the cumulative variance of these components to see how much of the data information the projection is preserving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7, 5))\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components', fontdict = fontdict, labelpad = 10)\n",
    "plt.ylabel('Cumulative Explained Variance', fontdict = fontdict, labelpad = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 10, figsize = (16, 4),\n",
    "                       subplot_kw = {'xticks': [], 'yticks': []},\n",
    "                       gridspec_kw = {'hspace': 0.1, 'wspace': 0.1})\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap = 'binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap = 'binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('Full-dim\\nInput', fontdict = fontdict, labelpad = 10)\n",
    "ax[1, 0].set_ylabel('150-dim\\nReconstruction', fontdict = fontdict, labelpad = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features. Although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image. What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about the mathematics of PCA for 2D images <a href='https://towardsdatascience.com/face-dataset-compression-using-pca-cddf13c63583'>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "SVD is a data summary method similar to PCA. It extracts important features from data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "df = pd.read_csv(url, names = ['sepal length', 'sepal width', 'petal length', 'petal width', 'target'])\n",
    "\n",
    "# Selecting features\n",
    "columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "data = df[columns]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take two singular values\n",
    "n = 2 \n",
    "\n",
    "# Performing SVD\n",
    "U, s, V = svd(data)\n",
    "\n",
    "# eye() creates a matrix with ones on the diagonal and zeros elsewhere\n",
    "sigma = np.mat(np.eye(n) * s[:n])\n",
    "\n",
    "# Making a dataframe with singular values and corresponding targets\n",
    "singular_values = U[:, :n]\n",
    "singular_values = pd.DataFrame(singular_values)\n",
    "singular_values.columns = ['SV1', 'SV2']\n",
    "singular_values['target'] = df['target']\n",
    "\n",
    "singular_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5)) \n",
    "plt.xlabel('Singular Value 1', fontdict = fontdict, labelpad = 10) \n",
    "plt.ylabel('Singular Value 2', fontdict = fontdict, labelpad = 10) \n",
    "plt.title('Singular Value Decomposition', fontdict = fontdict, pad = 10) \n",
    "\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    \n",
    "    indices_to_keep = singular_values['target'] == target\n",
    "    plt.scatter(singular_values.loc[indices_to_keep, 'SV1'], \n",
    "                singular_values.loc[indices_to_keep, 'SV2'], \n",
    "                c = color, \n",
    "                s = 50)\n",
    "    \n",
    "plt.legend(targets)\n",
    "plt.grid(b = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the output with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.copy()\n",
    "y = df.loc[:, ['target']].values\n",
    "\n",
    "# Standardizing the data\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "x = pd.DataFrame(x, columns = columns)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pricipal components\n",
    "x_pca = PCA(n_components = 4).fit_transform(x)\n",
    "x_pca = pd.DataFrame(x_pca, columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "x_pca['Target'] = y\n",
    "                 \n",
    "x_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5)) \n",
    "plt.xlabel('Principal Component 1', fontdict = fontdict, labelpad = 10) \n",
    "plt.ylabel('Principal Component 2', fontdict = fontdict, labelpad = 10) \n",
    "plt.title('2 Component PCA', fontdict = fontdict, pad = 10) \n",
    "\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    \n",
    "    indicesToKeep = x_pca['Target'] == target\n",
    "    plt.scatter(x_pca.loc[indicesToKeep, 'PC1'], \n",
    "                x_pca.loc[indicesToKeep, 'PC2'], \n",
    "                c = color, \n",
    "                s = 50)\n",
    "    \n",
    "plt.legend(targets)\n",
    "plt.grid(b = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with sparsity\n",
    "Sparse matrices are computationally expensive because of the large amount of redundant zero’s that are present in the matrix structure. Machine learning algorithms will not work as effectively as expected due to the increasing size and possibly due to the lack of exhaustive resources. \n",
    "\n",
    "The other significant issues caused are the decreasing speed times to effectively compute the matrix and a decrease in the computational processing speed of machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_data = np.array([[0,0,0,5,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [6,0,0,0,0,0,0,0],\n",
    "                        [0,0,7,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,8,0,0],\n",
    "                        [0,9,0,0,0,0,0,0]])\n",
    "\n",
    "sparse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR - Compressed Sparse Row Matrix\n",
    "It is a matrix respresentation for sparse matrices, where the zero entries are discarded and the non-zero entries are stored in a key-value pair with keys are tuples of row index and column index, while the values are the entry in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr = csr_matrix(sparse_data)\n",
    "print('Compressed Sparse Row Matrix', csr, sep = '\\n')\n",
    "\n",
    "ori_matrix = csr.todense()\n",
    "print('\\nOriginal Matrix', ori_matrix, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with ratings data\n",
    "We will use the MovieLens 100K public data set. The training file contains 100,000 ratings, by 943 users on 1,682 items. For the scope of this analysis we will ignore the timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "data = pd.read_csv('ratings.data', sep=\"\\t\", header = None, engine = 'python')\n",
    "\n",
    "# Defining column names and drooping the timestamp column\n",
    "data.columns = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "data.drop([\"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# Viewing the data\n",
    "display(data.head())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the user_id and the movie_id variables are categorical, we need to one-hot encode them. But before doing that let's first create a function to calculate the memory usage by the data frame before and after the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bytes to MB conversion factor\n",
    "BYTES_TO_MB_DIV = 0.000001\n",
    "\n",
    "# Defining a function to calculate memory usage\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    \n",
    "    memory = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3) \n",
    "    print(\"Memory usage is\", memory, \"MB\")\n",
    "\n",
    "print('Original data frame')\n",
    "print_memory_usage_of_data_frame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "We start by one-hot encoding the user_id and the movie_id data frames using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding the user_id and movie_id columnsusing Pandas\n",
    "data_encoded = pd.get_dummies(data, columns = ['user_id', 'movie_id'])\n",
    "\n",
    "# Viewing the data\n",
    "display(data_encoded.head())\n",
    "\n",
    "print('\\nOne-hot encoded data frame\\n')\n",
    "print(f'Shape is {data_encoded.shape}')\n",
    "print_memory_usage_of_data_frame(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse one-hot encoding with Pandas\n",
    "Now, we encode the data as sparse arrays using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse one-hot encoding the user_id and movie_id columns\n",
    "pd_data_sparse = pd.get_dummies(data, columns = ['user_id', 'movie_id'], sparse = True)\n",
    "\n",
    "# Viewing the data\n",
    "display(pd_data_sparse.dtypes)\n",
    "\n",
    "print('Pandas sparse one-hot encoded data frame')\n",
    "print_memory_usage_of_data_frame(pd_data_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse one-hot encoding with Scipy\n",
    "Now, we encode the data as sparse arrays using Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a one-hot data frame to a scipy sparse matrix\n",
    "def data_frame_to_scipy_sparse_matrix(df):\n",
    "    \n",
    "    arr = lil_matrix(df.shape, dtype = np.float32)\n",
    "    for i, col in enumerate(df.columns):\n",
    "        ix = df[col] != 0\n",
    "        arr[np.where(ix), i] = 1\n",
    "\n",
    "    return arr.tocsr()\n",
    "\n",
    "# Function to get the memory usage of a sparse matrix\n",
    "def get_csr_memory_usage(matrix):\n",
    "    \n",
    "    memory = (X_csr.data.nbytes + X_csr.indptr.nbytes + X_csr.indices.nbytes) * BYTES_TO_MB_DIV\n",
    "    print(\"Memory usage is\", memory, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['rating']\n",
    "\n",
    "# One-hot encoded data\n",
    "X = data_encoded[data_encoded.columns.difference(['rating'])]\n",
    "print('One-hot encoded X')\n",
    "print_memory_usage_of_data_frame(X)\n",
    "\n",
    "# Sparse one-hot encoded data using Pandas\n",
    "X_sparse = pd_data_sparse[pd_data_sparse.columns.difference(['rating'])]\n",
    "print('\\nPandas sparse one-hot encoded X')\n",
    "print_memory_usage_of_data_frame(X_sparse)\n",
    "\n",
    "# Sparse one-hot encoded data using Scipy\n",
    "X_csr = data_frame_to_scipy_sparse_matrix(X_sparse)\n",
    "print('\\nScipy sparse one-hot encoded X')\n",
    "get_csr_memory_usage(X_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comapring training speeds\n",
    "We'll compare the training speeds of a classification model using these encoded matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = {'Pandas data frame': [X, y],\n",
    "               'Sparse pandas data frame': [X_sparse, y],\n",
    "               'Scipy sparse matrix': [X_csr, y]}\n",
    "\n",
    "for key, item in vector_dict.items():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    start = time()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(item[0], item[1], test_size = 0.3, random_state = 42)\n",
    "    end = time()\n",
    "    \n",
    "    duration = round(end-start, 2)\n",
    "    print(\"Train-test split:\", duration, \"seconds.\")\n",
    "    \n",
    "    start = time()\n",
    "    model = LogisticRegression(random_state = 0, multi_class = 'auto', solver = 'liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    duration = round(end-start, 2)\n",
    "    print('Training:', duration, 'seconds.', '\\n' if key != 'Scipy sparse matrix' else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Model\n",
    "Here we implement the Transform Model. We find a dictionary whose product with a sparse 'code' closely approximates the data vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "n_samples = 100           # Number of data points\n",
    "n_components = 15         # Length of the encoding\n",
    "n_features = 20           # Number of features in the data vector\n",
    "n_nonzero_coefs = 10      # Number of nonzero coefficients in the encoding\n",
    "\n",
    "# Generating sample data\n",
    "X, dictionary, code = make_sparse_coded_signal(n_samples = n_samples, \n",
    "                                               n_components = n_components, \n",
    "                                               n_features = n_features, \n",
    "                                               n_nonzero_coefs = n_nonzero_coefs, \n",
    "                                               random_state = 42)\n",
    "\n",
    "# Asserting that our expectations about array dimensions hold\n",
    "assert X.shape == (n_features, n_samples)\n",
    "assert dictionary.shape == (n_features, n_components)\n",
    "assert code.shape == (n_components, n_samples)\n",
    "assert np.sum(code != 0, axis = 0).all() == np.full(shape = n_samples, fill_value = n_nonzero_coefs).all()\n",
    "assert X.all() == (dictionary @ code).all()\n",
    "\n",
    "print('Shape of the data array is', X.shape)\n",
    "print('X =', X, sep = '\\n')\n",
    "print('\\nShape of the dictionary array is', dictionary.shape)\n",
    "print('Dictionary =', dictionary, sep = '\\n')\n",
    "print('\\nShape of the code array is', code.shape)\n",
    "print('Code =', code, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dictionary learner and fit to the generated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_learner = DictionaryLearning(n_components = n_components, \n",
    "                                  transform_algorithm = 'lasso_lars', \n",
    "                                  random_state = 42)\n",
    "\n",
    "X_encoded = dict_learner.fit_transform(X.T).T\n",
    "\n",
    "print('Shape of encoded data is', X_encoded.shape)\n",
    "print('X_encoded =', X_encoded, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the level of sparsity of X_encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sparsity of X_encoded is', np.mean(X_encoded == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the average squared Euclidean norm of the reconstruction error of the sparse coded signal relative to the squared Euclidean norm of the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predicted = dict_learner.components_.T @ X_encoded\n",
    "round(np.mean(np.sum((X_predicted - X) ** 2, axis = 0) / np.sum(X ** 2, axis = 0)), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesis Dictionary Model\n",
    "Here we implement the Synthesis Dictionary Model. We find a sparse representation of data given a fixed dictionary. The goal is to find a sparse array, 'code', such that $$X\\approx code * dictionary$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1, -1], [0, 0, 3]], dtype = np.float64)\n",
    "dictionary = np.array([[0, 1, 0], [-1, -1, 2], [1, 1, 1], [0, 1, 1], [0, 2, 1]], dtype = np.float64)\n",
    "\n",
    "coder = SparseCoder(dictionary = dictionary, \n",
    "                    transform_algorithm = 'lars',\n",
    "                    transform_n_nonzero_coefs = 2)\n",
    "code = coder.transform(X)\n",
    "\n",
    "print('Dictionary =', dictionary, sep = '\\n')\n",
    "print('Code =', code, sep = '\\n')\n",
    "print('X =', X, sep = '\\n')\n",
    "print('Approximation =', code @ dictionary, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth = 120)\n",
    "\n",
    "X = np.random.randint(low = -10, high = 10, size = (6, 8))\n",
    "dictionary = np.random.randint(low = -10, high = 10, size = (12, X.shape[1]))\n",
    "\n",
    "X = X.astype(np.float64)\n",
    "dictionary = dictionary.astype(np.float64)\n",
    "\n",
    "distances = []\n",
    "\n",
    "print('Dictionary =', dictionary, sep = '\\n', end = '\\n\\n')\n",
    "\n",
    "for i in range(X.shape[0]+1):\n",
    "\n",
    "    if i != 0:\n",
    "        print()\n",
    "    \n",
    "    print('Number of non-zero coefficients per column =', i, end = '\\n\\n')\n",
    "\n",
    "    coder = SparseCoder(dictionary = dictionary, \n",
    "                        transform_algorithm = 'lars',\n",
    "                        transform_n_nonzero_coefs = i)\n",
    "    code = coder.transform(X)\n",
    "\n",
    "    print('Code =', np.around(code, 4), sep = '\\n')\n",
    "    print('X =', X, sep = '\\n')\n",
    "    approximation = code @ dictionary\n",
    "    print('Approximation =', np.around(approximation, 4), sep = '\\n')\n",
    "    distance = np.sqrt(np.mean(np.square(np.ravel(approximation) - np.ravel(X))))\n",
    "    print('Distance =', round(distance, 4))\n",
    "                    \n",
    "    distances.append(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'see how the average Euclidean distance between the elements of the original array and those of the approxiation varies with the number of non-zero coefficients allowed per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7, 5))\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Number of Non-zero Coefficients', fontdict = fontdict, labelpad = 10)\n",
    "plt.ylabel('Distance', fontdict = fontdict, labelpad = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitfd92962200d74e8ba4e1089743d69310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
